from __future__ import annotations

import json
import time
from datetime import datetime

import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_precision,
    context_recall,
)
from ragas.run_config import RunConfig

from config import CURRENT_CONFIG
from utils import load_embeddings, load_eval_llm, load_env

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_env()


# ===============================
# Loaders
# ===============================
def load_dataset_from_jsonl() -> Dataset:
    """JSONL íŒŒì¼ì—ì„œ RAGAS í‰ê°€ìš© Dataset ìƒì„± (êµ¬ë²„ì „/ì‹ ë²„ì „ í¬ë§· ëª¨ë‘ ì§€ì›)"""
    jsonl_path = CURRENT_CONFIG.rag_answers_path
    if not jsonl_path.exists():
        raise FileNotFoundError(f"JSONL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")

    rows = []
    with jsonl_path.open("r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data = json.loads(line)
                
                # êµ¬ë²„ì „ í¬ë§·(question, answer, ground_truth, contexts)ì„
                # ì‹ ë²„ì „ í¬ë§·(user_input, response, reference, retrieved_contexts)ìœ¼ë¡œ ë³€í™˜
                if "question" in data:
                    data["user_input"] = data.pop("question")
                if "answer" in data and "response" not in data:
                    data["response"] = data["answer"]
                if "ground_truth" in data:
                    data["reference"] = data.pop("ground_truth")
                if "contexts" in data:
                    data["retrieved_contexts"] = data.pop("contexts")
                
                rows.append(data)

    print(f"ğŸ“‚ JSONL ë¡œë“œ ì™„ë£Œ: {len(rows)}ê°œ ìƒ˜í”Œ")
    return Dataset.from_list(rows)


# ===============================
# Checkpoint ê´€ë¦¬
# ===============================
def load_checkpoint():
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ (ì´ë¯¸ í‰ê°€ ì™„ë£Œëœ ì¸ë±ìŠ¤ ëª©ë¡)"""
    checkpoint_path = CURRENT_CONFIG.checkpoint_path
    if checkpoint_path.exists():
        with checkpoint_path.open("r", encoding="utf-8") as f:
            return json.load(f)
    return {"completed_indices": [], "results": [], "start_time": None}


def save_checkpoint(checkpoint_data):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    log_dir = CURRENT_CONFIG.log_dir
    log_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_path = CURRENT_CONFIG.checkpoint_path
    with checkpoint_path.open("w", encoding="utf-8") as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)


def evaluate_single_sample(sample_data, embeddings, eval_llm, metrics, run_config):
    """ë‹¨ì¼ ìƒ˜í”Œ í‰ê°€"""
    dataset = Dataset.from_list([sample_data])
    result = evaluate(
        dataset,
        metrics=metrics,
        llm=eval_llm,
        embeddings=embeddings,
        run_config=run_config,
    )
    return result.to_pandas().iloc[0].to_dict()


# ===============================
# Main
# ===============================
def main():
    cfg = CURRENT_CONFIG
    print(f"{'='*60}")
    print(f"ğŸš€ RAGAS í‰ê°€ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    print(f"ì‹¤í—˜: {cfg.experiment_name}")
    print(f"í‰ê°€ ëª¨ë¸: {cfg.eval_model}")
    print(f"Log: {cfg.log_dir}\n")
    
    # 1) JSONLì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dataset_from_jsonl()
    total_samples = len(dataset)
    
    # 2) ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = load_checkpoint()
    completed_indices = set(checkpoint["completed_indices"])
    results = checkpoint["results"]
    
    if completed_indices:
        print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {len(completed_indices)}/{total_samples} ìƒ˜í”Œ ì™„ë£Œ")
        print(f"   â¡ï¸  {len(completed_indices)}ë²ˆë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.\n")
    else:
        print(f"ğŸ“Š ì „ì²´ {total_samples}ê°œ ìƒ˜í”Œ í‰ê°€ ì‹œì‘\n")
        checkpoint["start_time"] = datetime.now().isoformat()
    
    # 3) í‰ê°€ìš© ëª¨ë¸ ë¡œë“œ
    print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    embeddings = load_embeddings()
    eval_llm = load_eval_llm()
    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]
    run_config = RunConfig(timeout=300, max_retries=5, max_wait=60, max_workers=1)
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
    
    # 4) ìƒ˜í”Œë³„ í‰ê°€ (ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ê°œ)
    start_time = time.perf_counter()
    
    for idx in range(total_samples):
        if idx in completed_indices:
            continue  # ì´ë¯¸ ì™„ë£Œëœ ìƒ˜í”Œì€ ê±´ë„ˆë›°ê¸°
        
        sample = dataset[idx]
        print(f"[{idx+1}/{total_samples}] í‰ê°€ ì¤‘...")
        print(f"  ğŸ“ ì§ˆë¬¸: {sample['user_input'][:50]}...")
        
        try:
            sample_start = time.perf_counter()
            result = evaluate_single_sample(sample, embeddings, eval_llm, metrics, run_config)
            sample_elapsed = time.perf_counter() - sample_start
            
            # ê²°ê³¼ì— ì¸ë±ìŠ¤ ì¶”ê°€
            result["sample_idx"] = idx
            results.append(result)
            
            # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
            checkpoint["completed_indices"].append(idx)
            checkpoint["results"] = results
            save_checkpoint(checkpoint)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"  âœ… ì™„ë£Œ ({sample_elapsed:.1f}ì´ˆ)")
            print(f"     - faithfulness: {result.get('faithfulness', 'N/A'):.3f}")
            print(f"     - answer_relevancy: {result.get('answer_relevancy', 'N/A'):.3f}")
            print(f"     - context_precision: {result.get('context_precision', 'N/A'):.3f}")
            print(f"     - context_recall: {result.get('context_recall', 'N/A'):.3f}")
            print()
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"     ì¤‘ë‹¨ëœ ìœ„ì¹˜: {idx}ë²ˆ ìƒ˜í”Œ")
            print(f"     ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨. ì¬ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.\n")
            raise
    
    total_elapsed = time.perf_counter() - start_time
    
    # 5) ìµœì¢… ê²°ê³¼ ì €ì¥
    print(f"\n{'='*60}")
    print(f"âœ… ì „ì²´ í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")
    print(f"ìƒ˜í”Œë‹¹ í‰ê· : {total_elapsed/total_samples:.1f}ì´ˆ\n")
    
    cfg.log_dir.mkdir(parents=True, exist_ok=True)
    
    # CSV ì €ì¥
    df_results = pd.DataFrame(results)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = cfg.log_dir / f"ragas_results_{timestamp}.csv"
    df_results.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {out_csv}")
    
    # í‰ê·  ì ìˆ˜ ì¶œë ¥
    print(f"\n--- [í‰ê·  ì ìˆ˜] ---")
    for metric in ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]:
        if metric in df_results.columns:
            avg_score = df_results[metric].mean()
            print(f"{metric}: {avg_score:.4f}")
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ (ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ)
    checkpoint_path = cfg.checkpoint_path
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        print(f"\nğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")


if __name__ == "__main__":
    main()
