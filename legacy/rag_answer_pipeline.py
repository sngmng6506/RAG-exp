from __future__ import annotations

import gc
import json
import pandas as pd

try:
    import torch
except Exception:
    torch = None

from datasets import Dataset

try:
    from sentence_transformers import CrossEncoder
except Exception:
    CrossEncoder = None

from transformers import AutoModelForCausalLM, AutoTokenizer

from config import CURRENT_CONFIG
from utils import load_embeddings
from chunking_strategies import get_chunking_strategy


class RAGPipeline:
    def __init__(self):
        self.config = CURRENT_CONFIG
        self.embeddings = None
        self.chunking_strategy = None
        self.retriever = None
        self.reranker = None
        self._llm_model = None
        self._llm_tokenizer = None

    def load_embeddings(self):
        self.embeddings = load_embeddings()
        return self.embeddings
    
    def load_retriever(self):
        """ì²­í‚¹ ì „ëµì— ë”°ë¼ Retriever ë¡œë“œ"""
        if self.embeddings is None:
            self.load_embeddings()
        
        if self.chunking_strategy is None:
            self.chunking_strategy = get_chunking_strategy(self.config)
        
        self.retriever = self.chunking_strategy.get_retriever(self.embeddings)
        return self.retriever

    def load_reranker(self):
        if CrossEncoder is None:
            self.reranker = None
            return None
        model_path = self.config.reranker_model_path
        model_name = str(model_path) if model_path.exists() else "BAAI/bge-reranker-v2-m3"
        device = "cuda" if torch and torch.cuda.is_available() else "cpu"
        self.reranker = CrossEncoder(model_name, device=device)
        return self.reranker

    def load_llm(self, device: str = "cuda"):
        tokenizer = AutoTokenizer.from_pretrained(
            self.config.llm_path, trust_remote_code=True
        )
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        # bfloat16ì´ float16ë³´ë‹¤ ì•ˆì •ì  (NaN/inf ë¬¸ì œ ê°ì†Œ)
        model_dtype = torch.bfloat16 if device == "cuda" else torch.float32
        model = AutoModelForCausalLM.from_pretrained(
            self.config.llm_path,
            device_map="auto" if device == "cuda" else None,
            dtype=model_dtype,
            trust_remote_code=True,
        )
        self._llm_model = model
        self._llm_tokenizer = tokenizer
        return model

    def generate_answer(self, prompt: str) -> str:
        """model.generate()ë¥¼ ì§ì ‘ í˜¸ì¶œí•´ ì•ˆì •ì ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        inputs = self._llm_tokenizer(prompt, return_tensors="pt").to(self._llm_model.device)
        with torch.no_grad():
            outputs = self._llm_model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                do_sample=False,
                pad_token_id=self._llm_tokenizer.pad_token_id,
                eos_token_id=self._llm_tokenizer.eos_token_id,
            )
        generated = self._llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
        if "ë‹µë³€:" in generated:
            return generated.split("ë‹µë³€:")[-1].strip()
        return generated[len(prompt):].strip()

    def _rerank(self, question: str, contexts: list[str]) -> list[str]:
        if not self.reranker or not contexts:
            return contexts
        pairs = [(question, c) for c in contexts]
        scores = self.reranker.predict(pairs)
        ranked = sorted(zip(contexts, scores), key=lambda x: x[1], reverse=True)
        return [c for c, _ in ranked[: self.config.rerank_top_k]]

    def retrieve_contexts(self, question: str) -> list[str]:
        """ì²­í‚¹ ì „ëµì— ë”°ë¼ ë¬¸ë§¥ ê²€ìƒ‰"""
        if self.retriever is None:
            self.load_retriever()
        
        cfg = self.config
        
        # Retriever íƒ€ì…ì— ë”°ë¼ í˜¸ì¶œ ë°©ë²• ë‹¤ë¦„
        if hasattr(self.retriever, 'get_relevant_documents'):
            # ParentDocumentRetriever
            docs = self.retriever.get_relevant_documents(question)
            docs = docs[:cfg.retriever_top_k]
        else:
            # ì¼ë°˜ Retriever
            docs = self.retriever.invoke(question)
        
        contexts = [d.page_content for d in docs]
        return self._rerank(question, contexts)

    def build_rag_answer(self, question: str, contexts: list[str]) -> str:
        context_text = "\n\n".join([f"[{i+1}] {c}" for i, c in enumerate(contexts)])
        prompt = (
            "ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n\n"
            f"ë¬¸ë§¥:\n{context_text}\n\n"
            f"ì§ˆë¬¸: {question}\n"
            "ë‹µë³€:"
        )
        return self.generate_answer(prompt)

    def build_dataset(self, df: pd.DataFrame) -> Dataset:
        if self.reranker is None:
            self.load_reranker()
        if self._llm_model is None and self.config.generate_answers:
            self.load_llm()

        # í•„í„° ì¸ë±ìŠ¤ ë¡œë“œ (í‹€ë¦° ë¬¸í•­ë§Œ ì²˜ë¦¬)
        filter_indices = self._load_filter_indices()
        if filter_indices:
            print(f"\nğŸ” í•„í„° ëª¨ë“œ: {len(filter_indices)}ê°œ ë¬¸í•­ë§Œ ì²˜ë¦¬")
            print(f"   ì¸ë±ìŠ¤: {sorted(filter_indices)}")

        rows = []
        self.config.log_dir.mkdir(parents=True, exist_ok=True)
        log_path = self.config.rag_answers_path
        log_f = log_path.open("w", encoding="utf-8")

        for idx, row in df.iterrows():
            # í•„í„°ë§: í•„í„° ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ ì²˜ë¦¬
            if filter_indices and idx not in filter_indices:
                continue
            question = str(row["Q"]).strip()
            ground_truth = str(row["A"]).strip()
            contexts = self.retrieve_contexts(question)

            if self.config.generate_answers and self._llm_model is not None:
                answer = self.build_rag_answer(question, contexts)
            else:
                answer = ground_truth

            # RAGAS ìµœì‹  í¬ë§· (2024ë…„ í•˜ë°˜ê¸°~)
            record = {
                "user_input": question,
                "response": answer,
                "retrieved_contexts": contexts,
                "reference": ground_truth,
            }

            log_f.write(json.dumps(record, ensure_ascii=False) + "\n")
            rows.append(record)

        log_f.close()
        return Dataset.from_list(rows)

    def _load_filter_indices(self):
        """í‹€ë¦° ë¬¸í•­ ì¸ë±ìŠ¤ ë¡œë“œ (ìˆìœ¼ë©´)"""
        filter_path = self.config.incorrect_indices_path
        if filter_path.exists():
            with filter_path.open("r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data.get("incorrect_indices", []))
        return None
    
    def release_llm(self):
        if self._llm_model is None:
            return
        del self._llm_model
        del self._llm_tokenizer
        self._llm_model = None
        self._llm_tokenizer = None
        gc.collect()
        if torch and torch.cuda.is_available():
            torch.cuda.empty_cache()


def main():
    cfg = CURRENT_CONFIG
    print(f"{'='*60}")
    print(f"ğŸ”§ ì‹¤í—˜: {cfg.experiment_name}")
    print(f"{'='*60}")
    print(f"Collection: {cfg.collection_name}")
    
    # ì²­í‚¹ ì „ëµ ì •ë³´ ì¶œë ¥
    strategy = get_chunking_strategy(cfg)
    strategy_info = strategy.get_strategy_info()
    print(f"ğŸ“‹ ì²­í‚¹ ì „ëµ: {strategy_info['type']}")
    print(f"Retrieval: top_k={cfg.retriever_top_k}, rerank={cfg.rerank_top_k}")
    print(f"Log ì €ì¥: {cfg.log_dir}\n")
    
    if not cfg.xlsx_path.exists():
        raise FileNotFoundError(f"ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {cfg.xlsx_path}")

    pipeline = RAGPipeline()
    df = pd.read_excel(cfg.xlsx_path)
    dataset = pipeline.build_dataset(df)
    pipeline.release_llm()

    print(f"\nâœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(dataset)}ê°œ")
    print(f"ğŸ“‚ JSONL ì €ì¥: {cfg.rag_answers_path}")


if __name__ == "__main__":
    main()
