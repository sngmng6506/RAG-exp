from __future__ import annotations

from pathlib import Path
import json
from datetime import datetime

import pandas as pd

from config import CURRENT_CONFIG
from utils import load_eval_llm, load_env

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_env()


def judge_answer_correctness(llm, question: str, response: str, reference: str) -> dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨
    
    Returns:
        dict: {"is_correct": bool, "explanation": str}
    """
    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ë‹µë³€ì´ ì˜ë¯¸ì ìœ¼ë¡œ ê°™ì€ ë‚´ìš©ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ìƒì„±ëœ ë‹µë³€: {response}

ì •ë‹µ: {reference}

ë‘ ë‹µë³€ì´ ë³¸ì§ˆì ìœ¼ë¡œ ê°™ì€ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆìœ¼ë©´ "ì •ë‹µ", ë‹¤ë¥´ë©´ "ì˜¤ë‹µ"ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
í‘œí˜„ì´ ì¡°ê¸ˆ ë‹¤ë¥´ê±°ë‚˜ ì–´ë¯¸ê°€ ë‹¬ë¼ë„ í•µì‹¬ ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì •ë‹µì…ë‹ˆë‹¤.

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{{
    "judgment": "ì •ë‹µ" ë˜ëŠ” "ì˜¤ë‹µ",
    "explanation": "íŒë‹¨ ê·¼ê±°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ"
}}"""

    try:
        result = llm.invoke(prompt)
        content = result.content.strip()
        
        # JSON íŒŒì‹±
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        data = json.loads(content)
        is_correct = data.get("judgment", "ì˜¤ë‹µ") == "ì •ë‹µ"
        explanation = data.get("explanation", "íŒë‹¨ ê·¼ê±° ì—†ìŒ")
        
        return {
            "is_correct": is_correct,
            "explanation": explanation
        }
    except Exception as e:
        print(f"  âš ï¸ íŒë‹¨ ì˜¤ë¥˜: {e}")
        return {
            "is_correct": False,
            "explanation": f"íŒë‹¨ ì‹¤íŒ¨: {str(e)}"
        }


def calculate_ragas_avg(df: pd.DataFrame) -> dict:
    """RAGAS ë©”íŠ¸ë¦­ í‰ê·  ê³„ì‚°"""
    metrics = ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]
    avg_scores = {}
    
    for metric in metrics:
        if metric in df.columns:
            avg_scores[metric] = df[metric].mean()
    
    return avg_scores


def save_results_to_log(log_dir: Path, avg_scores: dict, acc: float, 
                        correct_count: int, total_count: int, 
                        details: list, csv_filename: str):
    """ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¡œê·¸ íŒŒì¼ ë° í‹€ë¦° ì¸ë±ìŠ¤ JSONìœ¼ë¡œ ì €ì¥"""
    log_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = log_dir / f"evaluation_log_{timestamp}.md"
    
    # í‹€ë¦° ë¬¸í•­ ì¸ë±ìŠ¤ ì €ì¥ (0-based)
    incorrect_indices = [d["idx"] for d in details if not d["is_correct"]]
    incorrect_path = log_dir / "incorrect_indices.json"
    with incorrect_path.open("w", encoding="utf-8") as f:
        json.dump({
            "incorrect_indices": incorrect_indices,
            "incorrect_count": len(incorrect_indices),
            "total_count": total_count,
            "accuracy": acc
        }, f, indent=2)
    
    print(f"\nâŒ í‹€ë¦° ë¬¸í•­ ì¸ë±ìŠ¤ ì €ì¥: {incorrect_path}")
    print(f"   í‹€ë¦° ë¬¸í•­ ìˆ˜: {len(incorrect_indices)}ê°œ")
    
    with log_path.open("w", encoding="utf-8") as f:
        f.write(f"# í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ\n\n")
        f.write(f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**í‰ê°€ ë°ì´í„°**: `{csv_filename}`\n\n")
        f.write(f"**ì´ ìƒ˜í”Œ ìˆ˜**: {total_count}ê°œ\n\n")
        
        f.write(f"---\n\n")
        f.write(f"## 1. RAGAS ë©”íŠ¸ë¦­ í‰ê·  ì ìˆ˜\n\n")
        f.write(f"| ë©”íŠ¸ë¦­ | í‰ê·  ì ìˆ˜ |\n")
        f.write(f"|--------|----------|\n")
        for metric, score in avg_scores.items():
            f.write(f"| {metric} | {score:.4f} |\n")
        
        f.write(f"\n---\n\n")
        f.write(f"## 2. LLM ê¸°ë°˜ ì •í™•ë„ í‰ê°€\n\n")
        f.write(f"- **ì •ë‹µ ê°œìˆ˜**: {correct_count}/{total_count}\n")
        f.write(f"- **ì •í™•ë„ (Accuracy)**: {acc:.2%}\n\n")
        
        f.write(f"---\n\n")
        f.write(f"## 3. ìƒ˜í”Œë³„ ìƒì„¸ ê²°ê³¼\n\n")
        
        for detail in details:
            idx = detail["idx"]
            question = detail["question"]
            response = detail["response"]
            reference = detail["reference"]
            is_correct = detail["is_correct"]
            explanation = detail["explanation"]
            
            status = "âœ… ì •ë‹µ" if is_correct else "âŒ ì˜¤ë‹µ"
            
            f.write(f"### ìƒ˜í”Œ {idx + 1}: {status}\n\n")
            f.write(f"**ì§ˆë¬¸**: {question}\n\n")
            f.write(f"**ìƒì„±ëœ ë‹µë³€**: {response}\n\n")
            f.write(f"**ì •ë‹µ**: {reference}\n\n")
            f.write(f"**íŒë‹¨ ê·¼ê±°**: {explanation}\n\n")
            f.write(f"---\n\n")
    
    print(f"\nğŸ“„ ìƒì„¸ ë¡œê·¸ ì €ì¥: {log_path}")
    return log_path


def main():
    cfg = CURRENT_CONFIG
    
    # CSV íŒŒì¼ ì°¾ê¸°
    csv_files = list(cfg.log_dir.glob("ragas_results_*.csv"))
    if not csv_files:
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cfg.log_dir}")
        return
    
    # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
    csv_path = max(csv_files, key=lambda p: p.stat().st_mtime)
    
    print(f"{'='*60}")
    print(f"ğŸ” LLM ê¸°ë°˜ ì •í™•ë„ í‰ê°€ ì‹œì‘")
    print(f"{'='*60}")
    print(f"ì‹¤í—˜: {cfg.experiment_name}")
    print(f"í‰ê°€ ëª¨ë¸: {cfg.eval_model}")
    print(f"CSV íŒŒì¼: {csv_path.name}\n")
    
    # 1. CSV ë¡œë“œ
    df = pd.read_csv(csv_path)
    total_samples = len(df)
    print(f"ğŸ“Š ì´ {total_samples}ê°œ ìƒ˜í”Œ ë¡œë“œ\n")
    
    # 2. RAGAS í‰ê·  ì ìˆ˜ ê³„ì‚°
    print(f"--- [RAGAS í‰ê·  ì ìˆ˜] ---")
    avg_scores = calculate_ragas_avg(df)
    for metric, score in avg_scores.items():
        print(f"{metric}: {score:.4f}")
    print()
    
    # 3. LLM ê¸°ë°˜ ì •í™•ë„ í‰ê°€
    print(f"{'='*60}")
    print(f"ğŸ¤– LLM ê¸°ë°˜ ì •í™•ë„ í‰ê°€ ì¤‘...")
    print(f"{'='*60}\n")
    
    llm = load_eval_llm()
    correct_count = 0
    details = []
    
    for idx, row in df.iterrows():
        question = row["user_input"]
        response = row["response"]
        reference = row["reference"]
        
        print(f"[{idx + 1}/{total_samples}] í‰ê°€ ì¤‘...")
        print(f"  ì§ˆë¬¸: {question[:50]}...")
        
        result = judge_answer_correctness(llm, question, response, reference)
        is_correct = result["is_correct"]
        explanation = result["explanation"]
        
        if is_correct:
            correct_count += 1
            print(f"  âœ… ì •ë‹µ")
        else:
            print(f"  âŒ ì˜¤ë‹µ")
        print(f"  ğŸ“ {explanation}\n")
        
        details.append({
            "idx": idx,
            "question": question,
            "response": response,
            "reference": reference,
            "is_correct": is_correct,
            "explanation": explanation
        })
    
    # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥
    accuracy = correct_count / total_samples
    
    print(f"\n{'='*60}")
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì •ë‹µ: {correct_count}/{total_samples}")
    print(f"ì •í™•ë„ (Accuracy): {accuracy:.2%}\n")
    
    # 5. ë¡œê·¸ ì €ì¥
    log_path = save_results_to_log(
        log_dir=cfg.log_dir,
        avg_scores=avg_scores,
        acc=accuracy,
        correct_count=correct_count,
        total_count=total_samples,
        details=details,
        csv_filename=csv_path.name
    )
    
    print(f"\nğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
